# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** Rickwan102
**Raw Score:** 45/50 (90.0%)
**Course Points Earned:** 100.0

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Great job. You correctly applied t-SNE to MNIST (using a 2k subset), colored by labels with a clear scatter, colorbar, and titles/axes. Parameters (init='pca', learning_rate='auto', random_state) are reasonable. Consider experimenting with perplexity or more samples for finer str

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you used the t-SNE embeddings with a KNN classifier, did a train/test split, and reported accuracy. This directly answers how it performs. Full credit.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good work: you used UMAP features and computed KNN accuracy. Code is coherent and should run given prior context. Minor tip: to avoid data leakage, fit UMAP on the training split only and transform the test split with the fitted reducer.

---

### Exercise 4 (16/20 = 80.0%)

**Part ex2-part1** (ex2-part1.code): 6/7 points

_Feedback:_ Good PCA+KNN pipeline and accuracy reported; components count shown. However, you fit PCA on the full X before the train/test split, causing data leakage. Fit PCA on X_train and transform X_test instead. Using 0.95 variance is fine. Plot step optional.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Good job applying UMAP: fit_transform, split, KNN, accuracy printed, and embedding dims reported. Approach is consistent with your PCA workflow. For best practice, consider fitting UMAP on train and transforming test, but no points off here.

**Part ex2-part3** (ex2-part3.answer): 3/6 points

_Feedback:_ Good intuition: UMAP preserves local neighborhoods, aiding KNN, while PCA is linear. However, your claim about PCA using only two components doesn’t match your prior setup (PCA(0.95)) and UMAP used 10 dims. No parameter exploration or mention of low n_neighbors.

---

### Exercise 1 (19/20 = 95.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Great job. You correctly applied PCA to 2 components and produced a clear 2D scatter plot colored by digit class with a proper colorbar and labels. This fully meets the requirements.

**Part pipeline-part2** (pipeline-part2.code): 3/4 points

_Feedback:_ Good attempt: you fit PCA on the data and plotted the first 40 values. However, you plotted cumulative explained variance, not the per-component variance typical of a scree plot. Plot explained_variance_ratio_[:40] (not cumsum) for full credit.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correctly computes cumulative explained variance from pca_full and finds the smallest number of components meeting the 95% threshold. Aligned with prior work and problem goal. Well done.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Excellent. You correctly used n_components_95, reduced the digit with PCA, and visualized the reconstruction alongside the original. This meets the task’s goal of visualizing the digit in the reduced space. Clear, correct, and aligned with prior work.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Good work. You trained KNN with and without PCA, preserved ~80% variance using PCA(0.80), transformed train/test correctly, compared accuracies, and reported components kept. Approach is correct and meets the objective.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:51:42 UTC

If you have questions about your grade, please reach out to the instructor.